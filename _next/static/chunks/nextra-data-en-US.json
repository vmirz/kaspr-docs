{"/docs/api-reference/v1alpha1":{"title":"API Reference","data":{"resources-types#Resources Types":"KasprApp\nKasprAgent\nKasprTask","kasprapp#KasprApp":"A program that runs components of a distributed stream processing application.","kasprappconfig#KasprAppConfig":"KasprApp configuration parameters.","kasprappspec#KasprAppSpec":"Specification of the desired settings of the application.","kafkaauthentication#KafkaAuthentication":"Kafka authentication configuration.","passwordsecret#PasswordSecret":"Kubernetes secret.","storagerequirements#StorageRequirements":"Disk storage configuration (disk)."}},"/docs/copyright":{"title":"Copyright","data":{"":"Kaspr Documentation (Copyright 2024-2025)Portions of this documentation are derived and adapted from the original Faust User Manual by Robinhood Markets, Inc.This material may be copied or distributed only subject to the terms and conditions set forth in the Creative Commons Attribution-ShareAlike 4.0 International http://creativecommons.org/licenses/by-sa/4.0/legalcode license.You may share and adapt the material, even for commercial purposes, but you must give the original author credit. If you alter, transform, or build upon this work, you may distribute the resulting work only under the same license or a license compatible to this one.Note:\nWhile the Kaspr documentation is offered under the Creative Commons Attribution-ShareAlike 4.0 International license the Kaspr software is offered under the BSD License (3 Clause)"}},"/docs/getting-started/architecture":{"title":"Architecture","data":{}},"/docs/getting-started/installation":{"title":"Installation","data":{"":"Learn how to install Kaspr Operator in a Kubernetes cluster."}},"/docs/getting-started/installation/helm":{"title":"Install with Helm","data":{"":"Use helm to install and manage kaspr-operator and related CRDs in your Kubernetes cluster.","prerequisites#Prerequisites":"Kubernetes 1.16+\nHelm 3.x","installation-modes#Installation Modes":"The Kaspr Operator can be installed in either cluster-scoped or namespace-scoped mode.\nWhen installed in cluster-scoped mode, the operator will watch for Kaspr resources across all namespaces.\nWhen installed in namespace-scoped mode, the operator will only watch for Kaspr resources in the same namespace where the operator is installed.\nBy default, the operator is installed in cluster-scoped mode. To install the operator in namespace-scoped mode, set the watchAnyNamespace option to false.Depending on which mode is used, either the ClusterRole or Role and ClusterRoleBinding or RoleBinding resources are created respectively.","installation-steps#Installation Steps":"","download-the-helm-chart#Download the helm chart":"git clone https://github.com/totalwinelabs/kaspr-helm.git\ncd kaspr-helm","install-the-operator#Install the operator":"The general syntax for helm installation is:\nhelm install <release> <chart> --namespace <namespace> --create-namespace [--set <other_parameters>]\nThe variables specified in the command are as follows:\n<chart> A path to a packaged chart, a path to an unpacked chart directory or a URL.\n<release> A name to identify and manage the Helm chart once installed.\n<namespace> The namespace in which the chart is to be installed.\nDefault configuration values can be changed using one or more --set <parameter>=<value> arguments. Alternatively, you can specify several parameters in a custom values file using the --values <file> argument.\nhelm install kaspr-operator charts/operator \\\n    --set operator.watchAnyNamespace=true \\\n    -n kaspr-operator \\\n    --create-namespace\nThis will install the operator in a dedicated kaspr-operator namespace using default options\nwhich is sufficient for most deployments. However, you can override options to customize the\ndeployment according to your needs. See values.yaml\nfor all possible configuration options.","custom-resource-definitions-crds#Custom Resource Definitions (CRDs)":"CRDs are included in the operator helm chart and will be automatically installed with the operator.\nHowever, they need to be manually updated or removed when necessary.\nUpdating CRDs\nkubectl apply -f ./charts/operator/crds\nDeleting CRDs\nkubectl delete -f ./charts/operator/crds","uninstall-the-operator#Uninstall the Operator":"helm uninstall kaspr-operator -n kaspr-operator\nHelm will not delete Kaspr CRDs. You will need to manually delete these if necessary."}},"/docs/getting-started/introduction":{"title":"Introduction","data":{"how-it-works#How it works":"","agents#Agents":"Process infinite streams in a straightforward manner. The concept of “agents” comes from the actor model, and means the stream processor can execute concurrently on on hundreds of machines at the same time.Deploy an agent using a Kubernetes resource and assign it to an app. Use regular Python or your favorite python libraries to customize stream processing logic and behavior.TODO: Update this example\napiVersion: kaspr.io/v1alpha1\nkind: KasprAgent\nmetadata:\n  name: my-agent\n  labels:\n    kaspr.io/app: my-app\nspec:\n    input:\n        topic: my-topic\n    processor:\n        pipeline:\n            - my-processor\n        operations:\n            - name: my-processor\n              map:\n                python: |\n                    def process(event):\n                    return event"}},"/docs":{"title":"Kaspr Documentation","data":{"":"Kaspr dramatically simplifies the creation of real-time, event-driven applications and microservices. Built as a set of Kubernetes CRDs (Custom Resource Definitions), it provides a straightforward framework for building scalable streaming and event processing systems.Kaspr has a single dependency: Kafka, which serves as the persistence and transport layer for all events processed by the system. While similar to Kafka Streams, Kaspr offers greater flexibility by not being limited to a domain-specific language (DSL). Kaspr is optimized for Kubernetes, offering both ease of use and scalability.","use-cases#Use Cases":"Event driven microservices\nReal-time data integration\nReal-time analytics\nStateful computations\nMaterialized views/cache","quick-start#Quick Start":""}},"/docs/user-guide/agents":{"title":"Agents - Distributed Stream Processors","data":{"":"An agent is a distributed system processing the events in a stream.","define-agents#Define Agents":"","inputs#Inputs":"A KasprAgent resource takes an input topic or channel that defines the source of events for the agent's stream.\nMultiple topicsYou can consume from multiple topics simultaniously by providing multiple topic names or a regex pattern. This is mostly useful when all inputs share the same data schema but can also be used in general.\napiVersion: kaspr.io/v1alpha1\nkind: KasprAgent\nmetadata:\n  name: welcome-email-sender\nspec:\n  inputs:\n    topic:\n      names:\n        - new-customers\nPartitioningWhen an agent reads from a topic, the stream is partitioned based on the key of the message. For example, the stream could have keys that are customer ids, and values that are purchases, then partitioning will decide that any message with the same customer id as key, is always delivered to the same agent instance.\nSometimes you’ll have to repartition the stream, to ensure a portion of the data is consistently delievered to the same agent instance on a particu. See Stream Processors for more information on the group_by operator.","app-assignment#App Assignment":"Agents must be assigned to a KasprApp. The assigned app is the designated worker that will host and run the agent stream processor.\napiVersion: kaspr.io/v1alpha1\nkind: KasprAgent\nmetadata:\n  name: offer-generator\n  kaspr.io/app: loyalty-app\nspec:\n  inputs:\n    topic:\n      names:\n        - customers","processors#Processors":"Processors take actions on events in a stream, processing one event at a time. Operations like filter, map, and groupBy are built-in processor action that transform the stream. You can also create custom operations using Python. An agent processor chains multiple operations together into a pipeline, with each input event flowing through the operations sequentially.There are two types of processor operations:\nTerminal: A terminal operator concludes the stream processing and represents the final operation in a pipeline.\nNon-terminal: A non-terminal operator transforms the stream and passes it along, enabling subsequent operators in the chain to process the data further."}},"/docs/user-guide/kafka-basics":{"title":"Kafka - Basics you need to know","data":{"":"Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable data processing. It enables applications to publish, subscribe to, and process streams of events (or messages) in real-time. Kafka stores these events in topics, which are partitioned and replicated for reliability.","topics#Topics":"A topic is a logical channel through which data is written and read in a Kafka cluster. Topics are partitioned, meaning the data is split across multiple segments (partitions) to enable parallel processing and scalability. Each partition is ordered, and messages within it are assigned sequential offsets. Topics are also replicated for fault tolerance, ensuring data availability even if a broker fails. Producers write messages to a topic, while consumers read from it, either individually or as part of a consumer group for load balancing. Kafka topics can be configured with retention policies to manage how long messages are stored, making them flexible for real-time and historical data processing.","partitions#Partitions":"A partition is a smaller, ordered subset of a Kafka topic, designed to enable scalability and parallelism in data processing. Each partition is identified by a unique number and stores messages in the order they are produced, with each message assigned a sequential offset. Partitions allow a topic to scale horizontally across multiple brokers, distributing both storage and processing load. Consumers in a consumer group can read from partitions in parallel, ensuring efficient data processing. Partitions also support replication for fault tolerance, with one partition replica designated as the leader to handle all reads and writes, while others act as backups.","offsets#Offsets":"An offset in Kafka is a unique identifier assigned to each message within a topic partition. Offsets ensure messages are stored and retrieved in the exact order they were produced. Consumers use offsets to track their position in the stream, allowing them to resume processing from a specific point if needed. Offsets are specific to a partition and are not shared across partitions, meaning each partition has its own independent sequence. This mechanism provides precise control over message consumption and ensures reliable processing in distributed systems.","consumers#Consumers":"A consumer is a client application that reads and processes messages from Kafka topics. Consumers subscribe to one or more topics and fetch data in sequential order from assigned partitions. They work as part of consumer groups, where each consumer in the group processes messages from a unique set of partitions, enabling parallelism and scalability. A KasprApp is a consumer group. Consumers use offsets to track which messages have been read, allowing them to resume processing from a specific point in case of a failure. This flexibility makes Kafka Consumers ideal for building event-driven and real-time data processing systems.","load-balancing#Load Balancing":"Load balancing and scale out is achieved through consumer groups, where multiple consumers collaboratively process messages from a topic. Each partition in the topic is assigned to only one consumer within the group at a time, ensuring that no two consumers handle the same data. If there are more partitions than consumers, some consumers may process multiple partitions, while if there are more consumers than partitions, some consumers remain idle.When a new consumer joins or leaves the group (or fails), Kafka triggers a rebalance to reassign partitions among the active consumers. This dynamic assignment enables horizontal scaling and fault tolerance. The rebalancing process ensures efficient load distribution, optimizing throughput while maintaining the order of messages within each partition.","log-compaction#Log Compaction":"Log compaction in Kafka is a mechanism that ensures only the most recent value for each unique key in a topic is retained, while older records with the same key are deleted. This process helps reduce storage usage and keeps topics efficient for use cases like maintaining the current state of a system. Compaction occurs in the background, allowing Kafka to store a snapshot of the latest data while retaining older data for keys that don’t have newer updates. Unlike traditional retention, which deletes all messages after a certain time or size threshold, log compaction preserves the latest state indefinitely, ensuring durability and correctness for stateful applications.A KasprTable uses log compaction to ensure table state can be recovered without a large space overhead."}},"/docs/user-guide/concepts":{"title":"Concepts","data":{"":"This section provides an overview of the concepts that are important to understand when working with Kaspr.","application-app#Application (App)":"An app (KasprApp) is a program that runs all the components of a distributed stream processing application.  It is composed of agents (stream processors), tasks, channels, tables, and web views to perform useful work.\nWe can have multiple instances of an app to distribute a processing and scale in a horizontal manner.An app can be seen as a service in a microservice architecture. It's common to have many different apps, each responsible for a set of functions, that are part of a larger or complex system.\n...","stream#Stream":"A stream is an infinite sequence of events coming from a Kafka topic or channel. In Kaspr, a stream is implicitly created through an (KasprAgent), which manages the stream's lifecycle, determines what enters the stream, and defines how its events are processed.An event serves as a general wrapper for messages. Each event contains references to both the serialized and deserialized versions of the key and value messages, along with additional metadata, such as the message offset.","agent#Agent":"An agent (KasprAgent) is a distributed system that processes events in a stream. Each agent runs within an app, and an app can host multiple agents. An agent consumes data from an input source, such as a Kafka topic or a channel, and performs one or more processing operations on either individual events or batches of events.Streams can be divided among agents either in a round-robin fashion or by partitioning them based on the message key. This determines how the stream is distributed across available agent instances within all app instances. For instance, if the stream's messages are keyed by account ID and the value is a high score, the partitioning ensures that all messages with the same account ID are consistently processed by the same agent instance.Agents are at the core of stream processing in Kaspr, capable of performing a variety of operations, including transformations and aggregations, right out of the box. Additionally, agents can define custom processing logic using arbitrary Python code, providing flexibility for more complex operations.","table#Table":"A (KasprTable) provides durable, fault-tolerant memory for stream processing. While similar to a database, a Table differs in key ways: instead of residing on a remote host and offering a rich query interface, a Table is a simple key-value store embedded directly within an application. This local embedding allows for ultra-fast reads and writes.Each Table is backed by a Kafka topic, often compacted and referred to as a changelog topic. Every record written to a Table is also published to its changelog topic. This design enables the system to rebuild the entire state of the Table in case of a failure, ensuring data consistency and fault tolerance.Internally, a Table leverages an embedded RocksDB database. The data capacity of RocksDB is limited by the disk size of the machine, not its memory, making it suitable for managing large datasets.Tables play a critical role in enabling applications to store state in a fault-tolerant manner, allowing stream processors to perform stateful computations such as aggregations and data enrichments.There are two types of Tables: normal and global.\nNormal:\nA normal Table is distributed across instances of an application, as it is partitioned based on the partitions of the underlying changelog topic. In a multi-instance setup, each application instance holds a subset of the Table's keys. However, in a single-instance setup, a normal Table behaves similarly to a global Table.\nGlobal:\nA global Table, by contrast, provides each application instance with a complete copy of the data. This distinction becomes important when scaling an application to run across multiple instances. Unlike normal Tables, which divide the dataset among instances, global Tables replicate the entire dataset to each instance.\nThis flexibility allows developers to choose the appropriate Table type based on their application's requirements for scalability and data locality.","task#Task":"A task (KasprTask) represents arbitrary work that is performed asynchronously in the background, independent of agents. Tasks can be defined to run as one-time operations, on fixed time intervals, or on a recurring schedule using loops or cron expressions.Tasks operate within an app, and an app can run multiple tasks simultaneously.Some examples of how tasks can be used include:\nPolling external APIs and publishing data to a topic or channel\nReading from a (KasprTable) and performing an action, such as making a POST request to an HTTP endpoint\nTriggering scheduled business processes\nGenerating synthetic data\nTasks provide a flexible way to perform background operations without blocking other processes within the app.\nTasks provide a powerful way to handle asynchronos operations in a kaspr application, enabling a wide variety of background processing needs.","web-view#Web View":"A webview (KasprWebView) connects web-based interactions with stream processing workflows. It allows you to expose your stream processing pipelines as HTTP endpoints, enabling external systems, applications, or users to interact with your data processing logic using standard web requests. By bridging HTTP-based communication with Kaspr's stream processing capabilities, WebViews provide a flexible and scalable way to integrate real-time data processing into web-accessible services.","channel#Channel":"A channel (KasprChannel) is an in-memory buffer or queue used for sending and receiving messages within a local application (process) only. In Kaspr, channels function similarly to Kafka topics, enabling communication between agents within the same app. However, unlike Kafka topics, messages in channels do not persist across application restarts, meaning they are temporary and are lost when the app is restarted."}},"/":{"title":"Kaspr – Event Streaming for Kubernetes","data":{}}}